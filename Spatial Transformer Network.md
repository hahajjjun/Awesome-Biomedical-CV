# Computer Vision Week 2. [2021 CV]

## Spatial Transformer Networks (NIPS 2015)

### References

[1] [https://kevinzakka.github.io/2017/01/10/stn-part1](https://kevinzakka.github.io/2017/01/10/stn-part1)

[2] Tensorflow KR PR12 : [Video Clip]. [https://www.youtube.com/watch?v=Rv3osRZWGbg](https://www.youtube.com/watch?v=Rv3osRZWGbg)

[3] [https://m.blog.naver.com/yl95yl/221589306754](https://m.blog.naver.com/yl95yl/221589306754)

[4] Pytorch : [Code]. [https://tutorials.pytorch.kr/intermediate/spatial_transformer_tutorial.html#id4](https://tutorials.pytorch.kr/intermediate/spatial_transformer_tutorial.html#id4)

### Introduction

<aside>
ğŸ”´ ì™œ spatial transformer networkê°€ í•„ìš”í• ê¹Œ?

</aside>

- CNNì€ spatial transformationì— ëŒ€í•´ invariantí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.
    - Scaling â‡’ X
    - Rotation â‡’ X
    - Translation â‡’ â–³
- poolingì„ í†µí•´ ì•½ê°„ì˜ translationì— ëŒ€í•´ì„œëŠ” invariantí•  ìˆ˜ ìˆë‹¤.
    - Max, Sum, Average poolingì€ í•´ë‹¹ ì˜ì—­ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.
    - í•˜ì§€ë§Œ Max-pooling layersë„ receptive fieldsê°€ fixed, localizedë˜ì–´ ìˆë‹¤.
- 2D Transformationì˜ ì¢…ë¥˜
    1. Rigid Transformation : *Rotation, Translation*
    2. Similarity Transformation : Rotation, Translation + *Scaling*
    3. Affine Transformation : Rotation, Translation, Scaling + *Shearing, Reflection*
    4. Homography (Projective Transformation) : Arbitrary Square â‡’ Arbitrary Square
        
        ![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled.png)
        
    5. TPS(Thin Plate Spline) : Spline Interpolationì˜ ì¼ì¢…, ì–‡ì€ ì²œ ìœ„ì˜ ì ë“¤ì´ ì²œì´ ì£¼ë¦„ì§ì— ë”°ë¼ ì¼ê·¸ëŸ¬ì§€ëŠ” ê²ƒê³¼ ê°™ì€ ë³€í™˜
        
        ![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%201.png)
        

---

<aside>
ğŸ”´ CNN vs Spatial transformer module

</aside>

> *This limitation of CNNs is due to having only a limited, pre-defined pooling mechanism for dealing with variations in the spatial arrangement of data.*
> 

> *Spatial transformer module is a dynamic mechanism that can actively spatially transform an image or a feature map.*
> 

![ë³€í˜•ëœ MNIST Datasetì— Spatial Transformerë¥¼ ì ìš©í•œ ê²°ê³¼](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_3.png)

ë³€í˜•ëœ MNIST Datasetì— Spatial Transformerë¥¼ ì ìš©í•œ ê²°ê³¼

- ì¼ë¶€ëŸ¬ ì°Œê·¸ëŸ¬ëœ¨ë¦¬ê±°ë‚˜, ë…¸ì´ì¦ˆë¥¼ ë„£ì€ ì´ë¯¸ì§€ input(a)ì´ spatial transformerë¥¼ ê±°ì³ canonicalí•œ ì´ë¯¸ì§€ output(c)ìœ¼ë¡œ ë³€í™˜ëœë‹¤.
- (b)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” predicted transformation functionì˜ í˜•íƒœëŠ” input dataì— ë”°ë¼ ì œê°ê° â‡’ dynamic mechanismì´ë‹¤.
- CNN ëª¨ë¸ì˜ end-to-end training ê³¼ì • ì†ì—ì„œ backpropagationì„ í†µí•´ transformation function(paramerter)ì„ í•œêº¼ë²ˆì— í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

---

### Architecture

- Spatial Transformer â‡’ Localisation network + Grid Generator + Sampler
- Spatial TransformerëŠ” input feature map(U)ë¥¼ ë°›ì•„ warped output feature map(V)ë¥¼ ë°˜í™˜í•œë‹¤.
    
    ![PNG á„‹á…µá„†á…µá„Œá…µ.png](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5.png)
    

---

<aside>
ğŸ”´ Localisation network

</aside>

- Regression Layerë¥¼ ë§ˆì§€ë§‰ ë‹¨ì— ê°€ì§€ê³  ìˆì–´ parameter  $\theta$ë¥¼ ë°˜í™˜í•˜ëŠ” neural network
- Fully Connected Network(FCN)ì´ë“  CNNì´ë“  ê´€ê³„ ì—†ìŒ
- input : feature map $U$, output : paratmer $\theta = f_{loc}(U)$

---

<aside>
ğŸ”´ Grid Generator

</aside>

- parameter $\theta$ ë¥¼ í†µí•´ transformation mapping $T_\theta(G)$ ìƒì„±
- $G = \{G_i\}: G_i = (x_i^t,y_i^t)$
- $\begin{bmatrix}x_i^s\\y_i^s\end{bmatrix} =T_{\theta}(G)\begin{bmatrix}x_i^t \\y_i^t \\1\end{bmatrix}$
    
    ![PNG á„‹á…µá„†á…µá„Œá…µ 4.png](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_4.png)
    

- *affine transformation*
    
    $T_{\theta}(G) = A_{\theta} = \begin{bmatrix}
    \theta_{11} & \theta_{12} & \theta_{13}\\
    \theta_{21} & \theta_{22} & \theta_{23} \end{bmatrix}$
    
    $\begin{bmatrix}x_i^s\\y_i^s\end{bmatrix} =\begin{bmatrix}
    \theta_{11} & \theta_{12} & \theta_{13}\\
    \theta_{21} & \theta_{22} & \theta_{23} \end{bmatrix}\begin{bmatrix}x_i^t \\y_i^t \\1\end{bmatrix}$
    
    ![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%202.png)
    

- *attention*
    
    $T_{\theta}(G) = A_{\theta} = \begin{bmatrix}
    s & 0 & t_x\\
    0 & s & t_y \end{bmatrix}$
    
    $\begin{bmatrix}x_i^s\\y_i^s\end{bmatrix} =\begin{bmatrix}
    s & 0 & t_x\\
    0 & s & t_y \end{bmatrix}\begin{bmatrix}x_i^t \\y_i^t \\1\end{bmatrix}$
    
    ![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%203.png)
    
- ì—¬ê¸°ì„œ  $T_{\theta}$ëŠ” $U$â†’$V$ë¡œ ê°€ëŠ” Transformationìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œë¡œ í–‰ë ¬ ê³„ì‚° ì‹œì—ëŠ” Canonical â†’ Distortedë¡œ ê°€ëŠ” Transformationì´ë¯€ë¡œ ë°˜ëŒ€ ë°©í–¥ì´ë‹¤.
- Transformation   $T_{\theta}$ ì˜ parameter $\theta$ë¡œ affine transformation, attention ì™¸ì—ë„ projective transformation, thin-plate-spline(TPS) tarnsformation ë“±ì„ ëª¨ë‘ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

---

<aside>
ğŸ”´ Sampler : Differentiable Image Sampling

</aside>

- SamplerëŠ” input feature map Uë¡œë¶€í„° sampled output feature map Vë¥¼ ìƒì„±í•œë‹¤.

![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%204.png)

- output feature map Vì˜ ì¢Œí‘œë§ˆë‹¤, Uì˜ ì–´ëŠ pointì—ì„œ ê°’ì„ ê°€ì ¸ì˜¬ì§€ $T_{\theta}(G)$ê°€ ê²°ì •í•œë‹¤.
- ì´ë•Œ, Vì˜ ì¢Œí‘œì— ëŒ€ì‘ë˜ëŠ” Uì˜ pointê°€ ì •ìˆ˜ ê²©ìì ì´ ì•„ë‹ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì¸ì ‘í•œ ì •ìˆ˜ ê²©ìì  ê°’ì˜ interpolationì„ í†µí•´ Vì˜ ê°’ì„ ê°€ì ¸ì˜¨ë‹¤.
    
    ![(3) interpolation ê³µì‹](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%205.png)
    
    (3) interpolation ê³µì‹
    
    ![PNG á„‹á…µá„†á…µá„Œá…µ 3.png](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_3%201.png)
    
- Interpolationì€ ì•„ë˜ì— ë‚˜ì™€ ìˆëŠ” ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ìˆ˜í–‰ ê°€ëŠ¥í•˜ë‹¤.

![(4) Nearest Integer Interpolation, (5) Bilinear Interpolation](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%206.png)

(4) Nearest Integer Interpolation, (5) Bilinear Interpolation

![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%207.png)

![ì œì¼ ê°€ê¹Œìš´ ê²©ìì  ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ë°©ë²•](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_2.png)

ì œì¼ ê°€ê¹Œìš´ ê²©ìì  ê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ë°©ë²•

![ì ì„ ë‘˜ëŸ¬ì‹¸ëŠ” ë„¤ ê°œì˜ ê²©ìì  ê°’ì˜ ì„ í˜• ê²°í•©](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_5.png)

ì ì„ ë‘˜ëŸ¬ì‹¸ëŠ” ë„¤ ê°œì˜ ê²©ìì  ê°’ì˜ ì„ í˜• ê²°í•©

- Loss ê°’ì„ Backpropagateí•˜ê¸° ìœ„í•´ì„œëŠ” interpolation functionì´ ë¯¸ë¶„ ê°€ëŠ¥í•´ì•¼ í•œë‹¤.
- Interpolation functionì´ ë¶ˆì—°ì†ì´ì–´ë„ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ ì„œ Backpropagation ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%208.png)

---

<aside>
ğŸ”´ Spatial Transformer Network

</aside>

- Spatial Transformer Networkë¼ëŠ” í•˜ë‚˜ì˜ ëª¨ë“ˆì„ CNN ì•ë‹¨ì— ê²°í•©í•´ì„œ Image ìì²´ë¥¼ ì‚¬ì „ì— transformí•˜ê±°ë‚˜, CNNì˜ layer ì‚¬ì´ì— ë¼ì›Œë„£ì–´ Convolution layerë¥¼ ê±°ì¹œ output feature mapì„ transformí•  ìˆ˜ë„ ìˆë‹¤.
- ê°œìˆ˜ì™€ êµ¬ì¡°ìƒì˜ ìœ„ì¹˜ëŠ” ì´ë¡ ì ìœ¼ë¡œ ì œí•œì´ ì—†ë‹¤.
- Spatial transformerê°€ ì–´ë–»ê²Œ input feature mapì„ transformí•  ì§€ëŠ” CNNì˜ ì „ì²´ cost functionì„ ìµœì†Œí™”í•˜ëŠ” training ê³¼ì • ì¤‘ì— í•™ìŠµë˜ë¯€ë¡œ, Training speedë¥¼ í¬ê²Œ ê°ì†Œì‹œí‚¤ì§€ ì•ŠëŠ”ë‹¤.
- ì˜¤íˆë ¤ Attentive modelì—ì„œëŠ” downsampling íš¨ê³¼ê°€ ìˆì–´ speedupí•˜ê¸°ë„ í•œë‹¤.
- STNì˜ ì—¬ëŸ¬ ê°€ì§€ í™œìš©ì— ëŒ€í•´ ì‹¤í—˜
    - Distorted MNIST datasetì— ì ìš©í•˜ì—¬ ì„±ëŠ¥í‰ê°€
    - STN moduleì„ CNNì˜ layer ì‚¬ì´ì— ì—¬ëŸ¬ ê°œ ë¼ì›Œë„£ëŠ” ê²½ìš°(ST-CNN multi model)
    - ë³‘ë ¬ì ìœ¼ë¡œ ì—¬ëŸ¬ ê°œ STN ë°°ì¹˜í•´ì„œ imageì˜ ë‹¤ë¥¸ ë¶€ë¶„ tracking
    - MNIST addition(ë‘ ê°œ ìˆ«ì ë™ì‹œì— ì¸ì‹), Co-localisation(ìˆ˜ì‹­ ê°œ ìˆ«ì ì¸ì‹) Experiment

### Experiments

<aside>
ğŸ”´ Distorted MNIST

</aside>

- MNIST handwriting datasetì— distortionì„ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì ìš©
    - *Rotation(R), Rotation/Scale/Translation(RTS), Projective Transformation(P), Elastic Warping(E)*
- Baseline FCN, CNN **vs** ST-FCN, ST-CNN(CNN : 2 max-pooling layers
- Spatial Transformer uses bilinear sampling, but different transformation functions
    - *Affine transformation(Aff), Projective transformation(Proj), 16-point thin plate spline transformation(TPS)*
- Approximately same parameters, trained with identical optimisation schemes(SGD, backprop, scheduled learning rate decrease, multinomial cross entropy loss)

![Model, Distortion ìœ í˜•ë³„ percentage error, ì™¼ìª½ì€ TPS(Thin Plate Spline), ì˜¤ë¥¸ìª½ì´ Aff(Affine Transformation)](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_6.png)

Model, Distortion ìœ í˜•ë³„ percentage error, ì™¼ìª½ì€ TPS(Thin Plate Spline), ì˜¤ë¥¸ìª½ì´ Aff(Affine Transformation)

- Spatial transformer enabled network outperforms its counterpart baseline network
- CNN ê³„ì—´ì´ FCNë³´ë‹¤ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì´ëŠ”ë°, ê·¸ ì´ìœ ëŠ” CNNì˜ max-pooling layerê°€ spatial invarianceì— ê¸°ì—¬í•˜ê³  convolutional layerê°€ ëª¨ë¸ë§ì„ ë” ì˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
- Spatial transformerë¥¼ ê²°í•©í•œ ëª¨ë¸ì´ ì„±ëŠ¥ í–¥ìƒë¨ : ST moduleì´ Spatial invarianceì— ê¸°ì—¬í•¨
- ì¶”ê°€ë¡œ noisy environment(background clutterë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” 60X60 MNIST digitsì´ë¯¸ì§€)ì—ì„œ FCN(13.2%), CNN(3.5%) > ST-FCN(2.0%), ST-CNN(1.7%) errorë¥¼ ì¤„ì„
    
    ![Noisy environment](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%209.png)
    
    Noisy environment
    
- TPS: most powerful, able to reduce error on elastically deformed digits(E), reduce complexity of the task, does not overfit on simpler data(ì˜ˆ: R)
- CNNìœ¼ë¡œ classifyí•˜ì§€ ëª»í•˜ëŠ” ê²ƒë“¤ë„ ST-CNNì´ classifyí–ˆë‹¤. (https://goo.gl/gdEhUu)
- ê·¸ë¦¬ê³  Transformationì€ ëª¨ë‘ ìˆ«ì ì´ë¯¸ì§€ë¥¼ "standard, upright posed digit"ìœ¼ë¡œ ë³€í™˜ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµë˜ì—ˆë‹¤.
    - Transformation ì´í›„ ì´ë¯¸ì§€ë¡œë¶€í„° classificationí•˜ëŠ” ê²ƒì´ ìµœì¢… Taskì¸ë°ë„, ê·¸ ê³¼ì •ì—ì„œ í•™ìŠµëœ transformationì€ upright poseë¡œ ë³€í™˜í•˜ë„ë¡ í•™ìŠµë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.

<aside>
ğŸ”´ Street View House Numbers(SVHN)

</aside>

- SVHN Datasetì€ 1ìë¦¬~5ìë¦¬ìˆ˜ì˜ ì§‘ í˜¸ìˆ˜ ì´ë¯¸ì§€(Real-world data), 20ë§Œì—¬ê°œ ë°ì´í„°ë¡œ êµ¬ì„±
- 64X64(Tight crop), 128X128 crop : image preprocessing
- ST : affine transformation, bilinear sampling kernels
    - ST-CNN Single : 4-layer CNNìœ¼ë¡œ localisation netì„ êµ¬ì„±í•œ CNN spatial transformerë¥¼ CNNì•ì— ë°°ì¹˜í•œ ëª¨ë¸
    - ST-CNN Multi : ì•„ë˜ ê·¸ë¦¼ì˜ (a)ì²˜ëŸ¼ CNNì˜ ì²« 4ê°œ convolutional layerì˜ ì• ë‹¨ì— 2-layer FCN spatial transformerë¥¼ í•˜ë‚˜ì”© ì‚½ì…í•œ ëª¨ë¸

![Maxout CNN, CNN, DRAM(Deep Recurrent Attention Model)ê³¼ ST-CNN(Single, Multi) ë¹„êµ](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%2010.png)

Maxout CNN, CNN, DRAM(Deep Recurrent Attention Model)ê³¼ ST-CNN(Single, Multi) ë¹„êµ

- Spatial transformer moduleì´ CNN architecture ì „ì— ê²°í•©í•˜ëŠ” ê²½ìš°(ST-CNN single)ì™€ ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ Spatial transformer moduleì´ CNNì˜ convolution layer ì‚¬ì´ì‚¬ì´ì— ìœ„ì¹˜í•˜ëŠ” ê²½ìš°(ST-CNN multi)ë¥¼ ë¹„êµí–ˆì„ ë•Œ 0.1% ê°ì†Œ, ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ì—ˆìŒ
    
    ![PNG á„‹á…µá„†á…µá„Œá…µ 7.png](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_7.png)
    
- ST-CNN multiì—ì„œ ë” ê¹Šì€ layerì— ìœ„ì¹˜í•œ Spatial transformerì˜ ê²½ìš°ì—ëŠ” enrichedëœ featureì— ëŒ€í•´ì„œ transformationì„ ì˜ˆì¸¡í•´ì•¼ í–ˆë‹¤.
- ì´ì „ì˜ SOTA modelì´ì—ˆë˜ DRAM(Deep Recurrent Attention Model)ê³¼ ë¹„êµí–ˆì„ ë•Œ 128px ì´ë¯¸ì§€ inputì— ëŒ€í•´ì„œëŠ” 4.5% â†’ 3.9%ë¥¼ ë‹¬ì„±í–ˆë‹¤.(ST-CNN single, multi)
    - DRAMì€ ensemble of models + monte carlo averagingë¥¼ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ ST-CNN modelì€ single modelì—ì„œ single forward passë§Œ í•˜ë©´ ì¶©ë¶„í•˜ë‹¤ê³  ì£¼ì¥í•œë‹¤.
    - resolution & network capacityë¥¼ transformerì—ì„œ crop/rescaleí•œ ë¶€ë¶„ì—ë§Œ ì§‘ì¤‘í•˜ë©´ ë˜ê³ , ST-CNN Multiê°€ ê¸°ì¡´ CNNë³´ë‹¤ 6%ë§Œ ëŠë ¤ì¡Œë‹¤ê³  ì£¼ì¥í•œë‹¤.

<aside>
ğŸ”´ Fine-Grained Classification

</aside>

- 200ì¢…ì˜ ìƒˆ ì‚¬ì§„ 11,788ì¥(6k training, 5.8k test)ìœ¼ë¡œ êµ¬ì„±ëœ Caltechì˜ CUB-200-2011 birds ë°ì´í„°ì…‹ì— fine-grained bird classificationì„ ì ìš©í•œ ì‹¤í—˜
- Baseline : Inception architectureì— batch normalisationì„ ì‚¬ìš©í•œ CNN êµ¬ì¡°
- spatial transformer 2ê°œ(2Ã—ST-CNN) ë˜ëŠ” 4ê°œ(4Ã—ST-CNN)ë¥¼ ë³‘ë ¬ë¡œ ì‚¬ìš©í•´ì„œ ìë™ìœ¼ë¡œ objectì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ attentioní•˜ê²Œ í•™ìŠµ
    - ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼, transformerëŠ” ê°ê¸° ë‹¤ë¥¸ image partë¥¼ captureí•˜ê³ , captureëœ ì´ë¯¸ì§€ëŠ” Inceptionì— ì˜í•´ initialize ë˜ì–´ ìˆëŠ” part description sub-netìœ¼ë¡œ ë“¤ì–´ê°
    - ê°ê° representationì„ ë„ì¶œí•´ì„œ concat â‡’ classified with single softmax layer

![PNG á„‹á…µá„†á…µá„Œá…µ 8.png](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_8.png)

![PNG á„‹á…µá„†á…µá„Œá…µ 9.png](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_9.png)

![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%2011.png)

- ëª¨ë“  ST-CNNì´ Baseline CNN ëª¨ë¸ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.
- red ë°•ìŠ¤ëŠ” ìƒˆì˜ head ë¶€ë¶„ì„, green ë°•ìŠ¤ëŠ” bodyì˜ ì¤‘ì‹¬ ë¶€ë¶„ì„ ì°¾ë„ë¡ ë³„ë„ì˜ supervision ì—†ì´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµë˜ì—ˆë‹¤(not explicitly defined, data-driven).

### Conclusion & Appendix

- Spatial Transformer â‡’ Self-contained module for neural networks
- Explicití•˜ê²Œ spatial transformationì„ ì§ì ‘ ìˆ˜í–‰í•´ì„œ inputìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ëŒ€ì‹ , end-to-endë¡œ spatial transformationì´ í•™ìŠµë˜ë„ë¡ í•œë‹¤.
- Loss Functionì— ëŒ€í•œ Modificationì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.
- SOTA baseline CNN modelë³´ë‹¤ task ìˆ˜í–‰ accuracy í–¥ìƒì‹œì¼°ë‹¤.
- MNIST addition(ë‘ ê°œ ìˆ«ì ë™ì‹œì— ì¸ì‹), Co-localisation(ìˆ˜ì‹­ ê°œ ìˆ«ì ì¸ì‹) Experiment, 3D transformationìœ¼ë¡œ í™•ì¥í–ˆë‹¤.

<aside>
ğŸ”´ MNIST Addition :  2-channel input handwriting dataë¥¼ ë³´ê³ ì„œ ë‘ ìˆ«ìì˜ í•©ì„ ì¶œë ¥

</aside>

- 2-channel inputì— ëŒ€í•´ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ST1, ST2ë¥¼ í•™ìŠµí•œë‹¤.
- ST1ì€ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Channel 1ì´ë¯¸ì§€ë¥¼ stabiliseí•˜ê³ , ST2ëŠ” ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼  Channel 2ì´ë¯¸ì§€ë¥¼ stabiliseí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
- Spatial Transformationì˜ indenpendencyë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

![Untitled](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/Untitled%2012.png)

- 2-channel Input ì´ë¯¸ì§€ë¥¼ í™œìš©í•´ ë…ë¦½ì ìœ¼ë¡œ ST1, ST2ë¥¼ í•™ìŠµì‹œí‚¨ í›„ cross-predictionì„ ì‹œì¼œì„œ 4-channel outputì„ ìƒì„±, ì´ê²ƒì„ concatenateí•´ì„œ FCNì„ í†µí•´ predictí•œë‹¤.
    
    ![PNG á„‹á…µá„†á…µá„Œá…µ 10.png](Computer%20Vision%20Week%202%20%5B2021%20CV%5D%203d0a2e1c6ce243dda744c73d00e0f0ba/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5_10.png)
    

<aside>
ğŸ”´ Co-localisation, Higher Dimensional Transformers

</aside>

[https://www.youtube.com/watch?v=Ywv0Xi2-14Y](https://www.youtube.com/watch?v=Ywv0Xi2-14Y)

<aside>
ğŸ”´ Spatial Transformation Network ì´í›„ì˜ ë…¼ë¬¸ë“¤

</aside>

### Deformable Convolutional Networks

[PR-002: Deformable Convolutional Networks (2017)](https://www.youtube.com/watch?v=RRwaz0fBQ0Y)