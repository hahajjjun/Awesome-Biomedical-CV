{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a09b53b",
   "metadata": {},
   "source": [
    "### FCN : Fully Convolutional Networks for Semantic Segmentation, CVPR 2015 \n",
    "*Editor : Junha Park*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dd6f6b4",
   "metadata": {},
   "source": [
    "### Related Works :  [OverFeat](https://arxiv.org/abs/1312.6229)\n",
    "- Shift-and-stitch trick yields dense predictions from coarse outputs without interpolation.\n",
    "\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/156198966-b352a7f3-27c4-4a10-9c74-5484d3c60f61.png\" width = \"500dp\"></img>\n",
    "\n",
    "<div align=\"center\">Fig 1. Shift-and-strich trick is sort of smoothing technique, which obtains <br/>output map with perturbations caused by slight offsets.</div>\n",
    "\n",
    "- What does 'coarse output' mean?\n",
    "    - Coarse output is generally low in resolution and small in size.\n",
    "    - Thus fine-grained and dense outputs are generally required, forcing coarse outputs to be high in resolution.\n",
    "    \n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/156199003-e1ba5a28-fc95-4ba5-85fb-be476e8b8e75.png\" width = \"500dp\"></img>\n",
    "<div align=\"center\">Fig 2. Expected result is fine-grained and dense, while output is coarse.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926eb2b1",
   "metadata": {},
   "source": [
    "### Contributions\n",
    "\n",
    "1. Deal with arbitrary input size (Alternative of patchwise training)\n",
    "2. Enhanced segmentation by exploiting both high and low-dimensional features\n",
    "3. Supervised pre-training with deep convents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed8347c1",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "1. Dense FC layer to 1X1 convolution for classification net\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/156199028-d214669a-dbeb-4e30-bc07-f138ffca351e.png\" width = \"500dp\"></img>\n",
    "<div align = \"center\">Fig 3. 1X1 convnet</div>\n",
    "- Convolution layers are working as FC layers.\n",
    "- Not only 1X1 convolutions, convolutions can replace arbitrary dimension of FC layers.\n",
    "- FC layer output is probability distribution, while 1X1 conv layer output is heatmap.\n",
    "- Heatmap can be interpreted as spatial set of pixel-wise probability distribution.\n",
    "2. Skip FCNs for upsampling\n",
    "- Since output heatmap has low resolution, upsampling is required.\n",
    "- Upsampling & output of downsampling layer are applied together.\n",
    "- We call it skip FCNs architecture. <br/>\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/156199067-4224f500-f822-4ec6-9c20-62b2b0714e77.png\" width = \"800dp\"></img>\n",
    "<div align = \"center\">Fig 4. Skip FCNs</div>\n",
    "\n",
    "```python\n",
    "# Pseudocode\n",
    "FCN-32s = pool5.output()\n",
    "FCN-16s = Upsample(FCN-32s) + pool4.output()\n",
    "FCN-8s = Upsample(FCN-16s) + pool3.output()\n",
    "```\n",
    "- Summation of upsampled output and previous downsampled output aids dense prediction. <br/>\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/156199084-d9b053e4-a468-487a-8e99-6fff0681a44e.png\" width = \"400dp\"></img>\n",
    "<div align = \"center\">Fig 5. Skip FCNs output</div>\n",
    "- Direct summation of downsampled feature maps enhances segmentation performance.\n",
    "- FCN-8s shows most dense prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fadf2c3",
   "metadata": {},
   "source": [
    "### Conceptual Intutitions\n",
    "1. What is the nature of FCN output?\n",
    "- Output of FCN with only downsamping layers is classification heatmap. \n",
    "- Each pixels of heatmap are equivalent to patch-wise classification result, which patch size is equivalent to recpetive field of ConvNet.\n",
    "- Thus, FCN can be used as pre-trained feature extractor.\n",
    "2. How upsampling is possible?\n",
    "- Conceputally, upsampling is deconvolving corase representations into fine-grained representations.\n",
    "- Generally, backward convolution(deconvolution) techniques are applied when upsampling, including interpolations.\n",
    "- [OverFeat](https://arxiv.org/abs/1312.6229) gave a hint about obtaining dense predictions without interpolations\n",
    "3. What does patchwise training of FCN mean?\n",
    "- Patchwise training means 'loss sampling', just like stochastic optimization.\n",
    "- By using ConvNet, created patches(same size with receptive field) significantly overlap, enables efficient computation.\n",
    "- Sampling in patchwise training can also correct class imbalance problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
